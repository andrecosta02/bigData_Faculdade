{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaVGfQW3dAIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002ed725-892f-4273-b4fb-2003d725aed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.35.58-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting botocore<1.36.0,>=1.35.58 (from boto3)\n",
            "  Downloading botocore-1.35.58-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.58->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading boto3-1.35.58-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.58-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.35.58 botocore-1.35.58 jmespath-1.0.1 s3transfer-0.10.3\n"
          ]
        }
      ],
      "source": [
        "pip install pandas pyarrow boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "# Configuração com as credenciais AWS\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id='---', # Removido para realizar Commit através do GitHub\n",
        "    aws_secret_access_key='---', # Removido para realizar Commit através do GitHub\n",
        "    region_name='us-east-1'  # Escolha sua região AWS\n",
        ")\n",
        "\n",
        "# Inicializar o cliente S3\n",
        "s3_client = session.client('s3')"
      ],
      "metadata": {
        "id": "7eM6rqi1eO2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"brsahan/data-science-job\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Um2GCbj3eG",
        "outputId": "0b2baec9-3d34-4c24-c42b-069db139719f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/brsahan/data-science-job/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from io import BytesIO\n",
        "\n",
        "# Exemplo de DataFrame de dados\n",
        "# data = {\n",
        "#     'nome': ['Alice', 'Bob', 'Charlie'],\n",
        "#     'idade': [24, 27, 22],\n",
        "#     'cidade': ['São Paulo', 'Rio de Janeiro', 'Curitiba']\n",
        "# }\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Converter DataFrame para tabela Parquet\n",
        "# table = pa.Table.from_pandas(df)\n",
        "\n",
        "\n",
        "\n",
        "# lendo o arquivo em csv\n",
        "df = pd.read_csv(path + \"/data_science_job.csv\")\n",
        "table = pa.Table.from_pandas(df)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "3he8M1fNenYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f534e7b-fdeb-442d-8d63-e514a5018836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      work_year                             job_title  job_category  \\\n",
            "0          2022  Machine Learning Engineer  in office      Analysis   \n",
            "1          2020                Statistician  (Remote)         ML/AI   \n",
            "2          2022           Machine Learning Engineer           ML/AI   \n",
            "3          2022               Data Analyst  in office         ML/AI   \n",
            "4          2020                        Statistician    Data Science   \n",
            "...         ...                                   ...           ...   \n",
            "4995       2020   Machine Learning Engineer  (Remote)           NaN   \n",
            "4996       2021   Machine Learning Engineer  (Remote)           NaN   \n",
            "4997       2022           Machine Learning Engineer        Analysis   \n",
            "4998       2021                        Statistician    Data Science   \n",
            "4999       2020              Data Scientist  (Remote)   Engineering   \n",
            "\n",
            "     salary_currency  salary  salary_in_usd employee_residence  \\\n",
            "0                EUR  186597         136086                 US   \n",
            "1                JPY  110630          67982                 JP   \n",
            "2                INR   61280         153309                 UK   \n",
            "3                JPY  154130         135242                 DE   \n",
            "4                EUR  172312          35156                 UK   \n",
            "...              ...     ...            ...                ...   \n",
            "4995             NaN  179769         179111                 UK   \n",
            "4996             NaN  184642         196373                 CN   \n",
            "4997             GBP  135319          51366                 UK   \n",
            "4998             EUR   58037         181817                 US   \n",
            "4999             GBP  143565         172425                 CN   \n",
            "\n",
            "     experience_level employment_type work_setting company_location  \\\n",
            "0                  MI              CT       Remote               DE   \n",
            "1                  EX              FL       Remote               IN   \n",
            "2                  MI              CT       Hybrid               CN   \n",
            "3                  SE              FT       Hybrid               MX   \n",
            "4                  MI              FT    In-person               UK   \n",
            "...               ...             ...          ...              ...   \n",
            "4995              NaN              CT    In-person               IN   \n",
            "4996              NaN              FL       Remote               UK   \n",
            "4997               EN              FL       Hybrid               JP   \n",
            "4998               MI              PT       Remote               DE   \n",
            "4999               EX              CT       Remote               CN   \n",
            "\n",
            "     company_size  \n",
            "0               L  \n",
            "1               M  \n",
            "2               L  \n",
            "3               L  \n",
            "4               S  \n",
            "...           ...  \n",
            "4995          NaN  \n",
            "4996          NaN  \n",
            "4997            M  \n",
            "4998            S  \n",
            "4999            L  \n",
            "\n",
            "[5000 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buffer em memória para armazenar o arquivo Parquet temporariamente\n",
        "buffer = BytesIO()\n",
        "pq.write_table(table, buffer)\n",
        "buffer.seek(0)  # Mover o cursor para o início do buffer\n",
        "\n",
        "# Nome do bucket e do arquivo no S3\n",
        "bucket_name = 'andre-victor-uninassau'\n",
        "file_path = 'pasta_destino/data_science_job.parquet'\n",
        "\n",
        "# Cria o bucket se ele nao existir\n",
        "try:\n",
        "  s3_client.create_bucket(Bucket=bucket_name)\n",
        "  print(f\"Bucket '{bucket_name}' criado com sucesso.\")\n",
        "except Exception as e:\n",
        "  print(f\"Erro ao criar o bucket '{bucket_name}': {e}\")\n",
        "  pass\n",
        "\n",
        "# Carregar o arquivo Parquet no S3\n",
        "s3_client.upload_fileobj(buffer, bucket_name, file_path)\n",
        "print(f\"Arquivo salvo no S3 em: s3://{bucket_name}/{file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZtB0TGP-B-X",
        "outputId": "e8c14119-674d-4f9c-d3d8-2d6fa85eefd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket 'andre-victor-uninassau' criado com sucesso.\n",
            "Arquivo salvo no S3 em: s3://andre-victor-uninassau/pasta_destino/data_science_job.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import boto3\n",
        "from io import BytesIO\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# Configurações do S3\n",
        "partitioned_output_path = 'data_science_job_parquet_partitioned/'  # Caminho dentro do bucket S3\n",
        "\n",
        "# Configurar os dados de exemplo\n",
        "# data = {\n",
        "#     'nome': ['Alice', 'Bob', 'Charlie'],\n",
        "#     'idade': [24, 27, 22],\n",
        "#     'cidade': ['São Paulo', 'Rio de Janeiro', 'Curitiba'],\n",
        "#     'data': [\n",
        "#         datetime.datetime(2023, 11, 10),\n",
        "#         datetime.datetime(2023, 11, 11),\n",
        "#         datetime.datetime(2023, 11, 12)\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# Carregar os dados em um DataFrame do Pandas\n",
        "df = pd.read_csv(path + \"/data_science_job.csv\")\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Extrair ano, mês e dia como colunas para particionamento\n",
        "# df['ano'] = df['data'].dt.year\n",
        "# df['mes'] = df['data'].dt.month\n",
        "# df['dia'] = df['data'].dt.day\n",
        "df['work_year'] = df['work_year']\n",
        "\n",
        "# Converter DataFrame para uma tabela do PyArrow\n",
        "table = pa.Table.from_pandas(df)\n",
        "\n",
        "# Função para salvar a tabela particionada em S3\n",
        "# def save_partitioned_to_s3(table, bucket_name, output_path, partition_cols=['ano', 'mes', 'dia']):\n",
        "def save_partitioned_to_s3(table, bucket_name, output_path, partition_cols=['work_year']):\n",
        "    # Cria o buffer de memória para armazenar o arquivo Parquet temporariamente\n",
        "\n",
        "\n",
        "    # Salva os dados particionados em um diretório temporário local\n",
        "    temp_output_path = \"/tmp/dados_parquet_partitioned\"\n",
        "    pq.write_to_dataset(\n",
        "        table,\n",
        "        root_path=temp_output_path,\n",
        "        partition_cols=partition_cols\n",
        "    )\n",
        "\n",
        "    # Para cada arquivo Parquet particionado, faça upload para o S3\n",
        "    for root, dirs, files in os.walk(temp_output_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".parquet\"):\n",
        "                local_file_path = os.path.join(root, file)\n",
        "                # Calcula o caminho relativo dentro do diretório temporário\n",
        "                relative_path = os.path.relpath(local_file_path, temp_output_path)\n",
        "                s3_file_path = os.path.join(output_path, relative_path)\n",
        "\n",
        "                # Carregar o arquivo Parquet para o S3\n",
        "                with open(local_file_path, 'rb') as f:\n",
        "                    s3_client.upload_fileobj(f, bucket_name, s3_file_path)\n",
        "                print(f\"Arquivo carregado no S3 em: s3://{bucket_name}/{s3_file_path}\")\n",
        "\n",
        "# Executar a função de salvamento particionado\n",
        "save_partitioned_to_s3(table, bucket_name, partitioned_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1iETCqP-Hq0",
        "outputId": "23cddacd-bdf9-4694-fb99-739c759ee6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo carregado no S3 em: s3://andre-victor-uninassau/data_science_job_parquet_partitioned/work_year=2020/60ccd63e877e4453bd08841c01176541-0.parquet\n",
            "Arquivo carregado no S3 em: s3://andre-victor-uninassau/data_science_job_parquet_partitioned/work_year=2021/60ccd63e877e4453bd08841c01176541-0.parquet\n",
            "Arquivo carregado no S3 em: s3://andre-victor-uninassau/data_science_job_parquet_partitioned/work_year=2022/60ccd63e877e4453bd08841c01176541-0.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98wlsvdg_GI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}